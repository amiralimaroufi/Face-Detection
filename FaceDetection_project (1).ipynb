{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmbPtsLpWhaJ"
      },
      "source": [
        "# Install packages\n",
        "# labelme:LabelMe is an open-source graphical annotation tool for image and video data\n",
        "# albumentation: albumentations is a Python library for image augmentation\n",
        "# opencv:OpenCV is a Python library that allows you to perform image processing and computer vision tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "WJ0edAffWhaM"
      },
      "outputs": [],
      "source": [
        "!pip install labelme tensorflow  opencv-python matplotlib albumentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vya-Hp3WhaQ"
      },
      "source": [
        "#  Collect Images Using OpenCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "q4EBxGVVWhaQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import uuid #A simple guide to generating random IDs using UUID in Python\n",
        "import cv2 #It's open_cv's modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "1LIfUgH8WhaS"
      },
      "outputs": [],
      "source": [
        "IMAGES_PATH = os.path.join('data','images') # here ; first we want to make \"data\" folder then we make \"images\" as file into \"data\" the we join this file and folder and save it in IMAGES_PATH\n",
        "number_images = 30 #the number of images which webcam shot from our face in definite time duration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "pNWFZ8KqWhaT"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(0) ## video capturing with webcam / cv2.VideoCapture: turn your webcam on if you have more than one webcam you can index it which webcam you want to work\n",
        "for imgnum in range(number_images): # difinite a for loop for take a shot from your reality webcam in exact time interval\n",
        "    print('Collecting image {}'.format(imgnum))\n",
        "    ret, frame = cap.read()   # cap.read : transfer the images into vector and vectorize it\n",
        "                              #after taking a shot from face : ret is a boolean variable that returns true if the frame is available.\n",
        "                              #frame is an image array vector captured based on the default frames per second\n",
        "    imgname = os.path.join(IMAGES_PATH,f'{str(uuid.uuid1())}.jpg')  # at first here we put special id for each picture with uuid and change the id to string and format it with jpg and transfer  and join it to the IMAGES_PATH in images file\n",
        "    cv2.imwrite(imgname, frame)  #  cv2.imwrite A function that is used for displaying a picture in a window\n",
        "    cv2.imshow('frame', frame)\n",
        "    time.sleep(0.5) # time.sleep : we use it to definite the time interval  of taking consecutively shot from face in different status 0.5 is the time that we have chosen here\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):#cv2. waitKey(1) & 0xFF==ord('q')This line means that when the user presses 'q' from the keyboard, our video will stop.\n",
        "        break     #The waitKey(0) function returns -1 when no input is made whatsoever. As soon the event occurs i.e. a Button is pressed it returns a 32-bit integer.\n",
        "                  #The 0xFF in this scenario is representing binary 11111111 a 8 bit binary, since we only require 8 bits to represent a character we AND waitKey(0) to 0xFF. As a result, an integer is obtained below 255.\n",
        "                  #ord(char) returns the ASCII value of the character which would be again maximum 255.\n",
        "                  #Hence by comparing the integer to the ord(char) value, we can check for a key pressed event and break the loop.###\n",
        "cap.release( )#Once our work with the video is done, it is required that we release the resources that we have initialized for our code. For eg, if the VideoCapture object is using the webcam, then while it is using it, no other process on your system can use the webcam\n",
        "cv2.destroyAllWindows() #close all windows at any time after exiting the script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B39u4Rd_WhaT"
      },
      "source": [
        "#  Annotate Images with LabelMe you can refer to this link to know how working with labelme\n",
        "# (https://datagen.tech/guides/image-annotation/labelme/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Od14LudLWhaU"
      },
      "outputs": [],
      "source": [
        "!labelme\n",
        " # now the labeme framework is openning and you can draw bounding box on images that you were shoting from your webcam first at data directory you have to make a label file next to the images file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LMBuD8iWhaU"
      },
      "source": [
        "# Review Dataset and Build Image Loading Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "GeSgP4s5WhaV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1NFGgMPWhaW"
      },
      "source": [
        "###  Load Image into TF Data Pipeline\n",
        "# tf.data: for transforming data to tensor shapes\n",
        "# A training step involves the following steps:\n",
        "# 1. File reading\n",
        "# 2. Fetch or parse data\n",
        "# 3. Data transformation\n",
        "# 4. Using the data to train the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "tOy2nennWhaX"
      },
      "outputs": [],
      "source": [
        "images = tf.data.Dataset.list_files('data\\\\images\\\\*.jpg') # tf.data.Dataset.list_files: first put the all images(*) into list then transform it into tensors due to the top reasons which is mentioned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeoyvQUMWhaX"
      },
      "outputs": [],
      "source": [
        "images.as_numpy_iterator().next() # .as_numpy_iterator().next(): we can check that transforming is complete with this code because we in numpy and tensor form we have iteration on elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TWaVRiX3WhaX"
      },
      "outputs": [],
      "source": [
        "def load_image(x):\n",
        "    byte_img = tf.io.read_file(x) # tf.io.read_file( filename, name=None ) Defined in generated file:Reads and outputs the entire contents of the input filename\n",
        "    img = tf.io.decode_jpeg(byte_img) #Decode a JPEG-encoded image to a uint8 tensor.\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "7kNCnWuOWhaY"
      },
      "outputs": [],
      "source": [
        "images = images.map(load_image) #map : implement the top function on  each element of tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "WCeg4m4zWhaY"
      },
      "outputs": [],
      "source": [
        "images.as_numpy_iterator().next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-5AVUvUWhaY"
      },
      "outputs": [],
      "source": [
        "type(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v3a0JleWhaY"
      },
      "source": [
        "#  View Raw Images with Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "yQ2b8pe9WhaZ"
      },
      "outputs": [],
      "source": [
        "image_generator = images.batch(4).as_numpy_iterator() #take the images and put them in batches here we definite 4 batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "d3Tqo2RrWhaZ"
      },
      "outputs": [],
      "source": [
        "plot_images = image_generator.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "cD8xhYTgWhaZ"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "for idx, image in enumerate(plot_images): #enumerate: back index and images to idx and image\n",
        "    ax[idx].imshow(image) ##imshow : display images\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l9cOszsWhaa"
      },
      "source": [
        "###  MANUALLY SPLT DATA INTO TRAIN TEST AND VAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sgzyUMHWhaa"
      },
      "outputs": [],
      "source": [
        "90*.7 # 63 to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCXQQZroWhab"
      },
      "outputs": [],
      "source": [
        "90*.15 # 14 and 13 to test and val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL8h-r4dWhab"
      },
      "source": [
        "### Move the Matching Labels to the specific img files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ibrgEQV0Whab"
      },
      "outputs": [],
      "source": [
        "for folder in ['train','test','val']: # here we definite train test val folders in data\n",
        "    for file in os.listdir(os.path.join('data', folder, 'images')):\n",
        "\n",
        "        filename = file.split('.')[0]+'.json' #name of the images which we split it by . and save it in list and we need firt element of with  index zero beacuse second element is format of file that we add it '.json' manually\n",
        "        existing_filepath = os.path.join('data','labels', filename)\n",
        "        if os.path.exists(existing_filepath):\n",
        "            new_filepath = os.path.join('data',folder,'labels',filename) #Move the Matching Labels to the specific img files\n",
        "            os.replace(existing_filepath, new_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guEKTAQjWhac"
      },
      "source": [
        "#  Apply Image Augmentation on Images and Labels using Albumentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5jDbBW0Whac"
      },
      "source": [
        "###  Setup Albumentations Transform Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "g3A1etnRWhac"
      },
      "outputs": [],
      "source": [
        "import albumentations as alb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "FNDPbHS_Whad"
      },
      "outputs": [],
      "source": [
        "augmentor = alb.Compose([alb.RandomCrop(width=450, height=450),\n",
        "                         alb.HorizontalFlip(p=0.5),\n",
        "                         alb.RandomBrightnessContrast(p=0.2),\n",
        "                         alb.RandomGamma(p=0.2),\n",
        "                         alb.RGBShift(p=0.2),\n",
        "                         alb.VerticalFlip(p=0.5)],\n",
        "                       bbox_params=alb.BboxParams(format='albumentations',\n",
        "                                                  label_fields=['class_labels']))# alb.compose: augement our images to generate  more data for training  by changind the size/flip/brightness/contrast/gamma/rgb of images\n",
        "                      #alb.BboxParams: we definite the bounding box which we drew with labelme\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4rgcCR-Whad"
      },
      "source": [
        "###  Load a Test Image and Annotation with OpenCV and JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-tFOKx8gWhad"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(os.path.join('data','train', 'images','ffd85fc5-cc1a-11ec-bfb8-a0cec8d2d278.jpg')) # first in data folder we make train test val file and in each file we make image and label files which labels are relating to the images\n",
        "# and the join the directories with each other and with \"imread\" read the images is train file and one of the file which name is \"ffd85fc5-cc1a-11ec-bfb8-a0cec8d2d278.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-3PiDIlsWhae"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join('data', 'train', 'labels', 'ffd85fc5-cc1a-11ec-bfb8-a0cec8d2d278.json'), 'r') as f:\n",
        "    label = json.load(f)  # here we work with labels of train images which format is json and we read json file with this code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CiU6K2dWhaz"
      },
      "outputs": [],
      "source": [
        "label['shapes'][0]['points'] # here is example that in json format we have shape and in subset of shape we need the first par with index zero which is related to coordinations of bounding box and the the points that we want which is our exact coordination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvbOLVHhWha0"
      },
      "source": [
        "### Extract Coordinates and Rescale to Match Image Resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "toSN70S1Wha0"
      },
      "outputs": [],
      "source": [
        "coords = [0,0,0,0] # first we definite list with 4 elements because we have four thing for bounding box \"x,y,w(width),h(height)\"\n",
        "coords[0] = label['shapes'][0]['points'][0][0] # now we change the each elements to related coordination here for example in label in shape part in json format and index zero related to coordination and point part we  have two by two matricies which each element represent each x,y,w,h\n",
        "coords[1] = label['shapes'][0]['points'][0][1]\n",
        "coords[2] = label['shapes'][0]['points'][1][0]\n",
        "coords[3] = label['shapes'][0]['points'][1][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRR-6VuEWha0"
      },
      "outputs": [],
      "source": [
        "coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ceQ63THwWha1"
      },
      "outputs": [],
      "source": [
        "coords = list(np.divide(coords, [640,480,640,480])) # now we normalize each coordination and by dividing and shorten the size of bbox and the save it into list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PWCEX5tWha1"
      },
      "outputs": [],
      "source": [
        "coords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g90Ug62EWha1"
      },
      "source": [
        "#  Apply Augmentations and View Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "WgTKYn8yWha1"
      },
      "outputs": [],
      "source": [
        "augmented = augmentor(image=img, bboxes=[coords], class_labels=['face']) # now we augment the pictures with class  'lable face' and bounding box list(coords) and img which is our images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "pBrBCIB3Wha2"
      },
      "outputs": [],
      "source": [
        "#augmented['bboxes'][0][:2] which augment our images which [0] is row one becuse in list(coords) we just have one row and [:2] it means the column one and coulumn two\n",
        "#augmented['bboxes'][0][2:] which augment our images which [0] is row one becuse in list(coords) we just have one row and [2:] it means the column three and coulumn four"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydGUNKGNWha2"
      },
      "outputs": [],
      "source": [
        "augmented['bboxes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6Sepq98iWha2"
      },
      "outputs": [],
      "source": [
        "cv2.rectangle(augmented['image'], # cv2.rectangle: draw rectangle bounding box on each augmented pictures automaticaly according to 30 pictures that we manually took a shot from webcam\n",
        "              tuple(np.multiply(augmented['bboxes'][0][:2], [450,450]).astype(int)),\n",
        "              tuple(np.multiply(augmented['bboxes'][0][2:], [450,450]).astype(int)),\n",
        "                    (255,0,0), 2)  ## notice that we should multipy by prior img scale to be showed by open_cv\n",
        "\n",
        "plt.imshow(augmented['image'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxXVKp3gWha2"
      },
      "source": [
        "#  Build and Run Augmentation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Q7CGCI2SWha3"
      },
      "outputs": [],
      "source": [
        "for partition in ['train','test','val']:\n",
        "    for image in os.listdir(os.path.join('data', partition, 'images')):\n",
        "    # The os. listdir() method returns a list of the names of the entries in a directory\n",
        "        img = cv2.imread(os.path.join('data', partition, 'images', image))\n",
        "\n",
        "        coords = [0,0,0.00001,0.00001]\n",
        "        label_path = os.path.join('data', partition, 'labels', f'{image.split(\".\")[0]}.json') # image.split(\".\")[0]: the label file and we split by . and save it into list which we need the index zero of that list\n",
        "        if os.path.exists(label_path): # we check that te label of each  images in correct file according to for loop in partition that we are in  with  os.path.exists if its true we read label path and definite coordinations pipline for all of the images\n",
        "            with open(label_path, 'r') as f:\n",
        "                label = json.load(f)\n",
        "\n",
        "            coords[0] = label['shapes'][0]['points'][0][0]\n",
        "            coords[1] = label['shapes'][0]['points'][0][1]\n",
        "            coords[2] = label['shapes'][0]['points'][1][0]\n",
        "            coords[3] = label['shapes'][0]['points'][1][1]\n",
        "            coords = list(np.divide(coords, [640,480,640,480]))\n",
        "\n",
        "        try:\n",
        "            for x in range(60): # now for each picture we  make 60 augmented pic from originial pic\n",
        "                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
        "                cv2.imwrite(os.path.join('aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])#imwrite() function that saves an image object to a specified file here we save augmented['image'] in this file'aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'\n",
        "\n",
        "                annotation = {} # now we definte annotation in dictionary form which includes such keys with names of bounding  box and (class of face: if the face is detected show 1 else show 0)\n",
        "                annotation['image'] = image\n",
        "\n",
        "                if os.path.exists(label_path): ## checking that related label exist in related file\n",
        "                    if len(augmented['bboxes']) == 0:\n",
        "                        annotation['bbox'] = [0,0,0,0]\n",
        "                        annotation['class'] = 0\n",
        "                    else:\n",
        "                        annotation['bbox'] = augmented['bboxes'][0] #the coordination of bounding box\n",
        "                        annotation['class'] = 1\n",
        "                else:\n",
        "                    annotation['bbox'] = [0,0,0,0]\n",
        "                    annotation['class'] = 0\n",
        "\n",
        "\n",
        "                with open(os.path.join('aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
        "                    json.dump(annotation, f) #dump function in Python is mainly used when we want to store and transfer objects (Python objects) into a file in the form of JSON\n",
        "                                              # here we store annotation that we definite on the above in 'f' file that we open it as f\n",
        "\n",
        "        except Exception as e: #'except Exception as e' statement is used with try-except statements to capture any exception raised in the try block and store it in an object named e\n",
        "            print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e63qIYkWWha3"
      },
      "source": [
        "###  Load Augmented Images to Tensorflow Dataset (ETL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "x-GqJcqsWha3"
      },
      "outputs": [],
      "source": [
        "train_images = tf.data.Dataset.list_files('aug_data\\\\train\\\\images\\\\*.jpg', shuffle=False)## (Extraction: we transform of train images into tensor pipeline that we have mentioned on the above)\n",
        "train_images = train_images.map(load_image)##(transforming to all images with map function)\n",
        "train_images = train_images.map(lambda x: tf.image.resize(x, (120,120)))# resize the pixel of images\n",
        "train_images = train_images.map(lambda x: x/255)## (load) #then normalization images by dividing by 255 and now we have pixel size between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ixiUW13lWha4"
      },
      "outputs": [],
      "source": [
        "test_images = tf.data.Dataset.list_files('aug_data\\\\test\\\\images\\\\*.jpg', shuffle=False) # for test images\n",
        "test_images = test_images.map(load_image)\n",
        "test_images = test_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
        "test_images = test_images.map(lambda x: x/255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "kOUyc15JWha4"
      },
      "outputs": [],
      "source": [
        "val_images = tf.data.Dataset.list_files('aug_data\\\\val\\\\images\\\\*.jpg', shuffle=False) #for val images\n",
        "val_images = val_images.map(load_image)\n",
        "val_images = val_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
        "val_images = val_images.map(lambda x: x/255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "E7iMrleHWha4"
      },
      "outputs": [],
      "source": [
        "train_images.as_numpy_iterator().next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCtQ55swWha5"
      },
      "source": [
        "#  Build Label Loading Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ZK4wnxBcWha6"
      },
      "outputs": [],
      "source": [
        "def load_labels(label_path): #Build Label Loading Function\n",
        "    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f: # we send the label path like:'aug_data\\\\train\\\\labels\\\\*.json' in function and open it( before we should change it to numpy format) and read it and encode it with utf-8\n",
        "        label = json.load(f)\n",
        "\n",
        "    return [label['class'], label['bbox']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01F3LC9uWha6"
      },
      "source": [
        "###  Load Labels to Tensorflow Dataset\n",
        "\n",
        "tensor flow graph for tf.py_fubction to have less complexity of computation\n",
        "(https://www.easy-tensorflow.com/files/1_2.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "lSg2dwi1Wha7"
      },
      "outputs": [],
      "source": [
        "train_labels = tf.data.Dataset.list_files('aug_data\\\\train\\\\labels\\\\*.json', shuffle=False) #train labels\n",
        "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16])) # tf.py_function:this function allows expressing computations in a TensorFlow graph as Python functions. In particular, it wraps a Python function func in a once-differentiable TensorFlow operation that executes it with eager execution enabled\n",
        "## mapping the unit8 type to labels and float16 to img to transform them into tf.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "tc8QBHtkWha7"
      },
      "outputs": [],
      "source": [
        "test_labels = tf.data.Dataset.list_files('aug_data\\\\test\\\\labels\\\\*.json', shuffle=False) # test labels\n",
        "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "li4ah8jOWha8"
      },
      "outputs": [],
      "source": [
        "val_labels = tf.data.Dataset.list_files('aug_data\\\\val\\\\labels\\\\*.json', shuffle=False) #val labels\n",
        "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHXDW3wAWha8"
      },
      "outputs": [],
      "source": [
        "train_labels.as_numpy_iterator().next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hn40264Wha8"
      },
      "source": [
        "#  Combine Label and Image Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC_9lgbqWha9"
      },
      "source": [
        "###  Create Final Datasets (Images/Labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "MRw8huhvWha9"
      },
      "outputs": [],
      "source": [
        "train = tf.data.Dataset.zip((train_images, train_labels)) #now we concat labels and images in each related file\n",
        "train = train.shuffle(5000)\n",
        "train = train.batch(8)\n",
        "train = train.prefetch(4)##Prefetching is the technique of loading the next batch of data into memory before the current batch has finished processing 4 is the number of next batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "njfiV0CFWha-"
      },
      "outputs": [],
      "source": [
        "test = tf.data.Dataset.zip((test_images, test_labels))\n",
        "test = test.shuffle(1300)\n",
        "test = test.batch(8)\n",
        "test = test.prefetch(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "iwDcEl3fWha-"
      },
      "outputs": [],
      "source": [
        "val = tf.data.Dataset.zip((val_images, val_labels))\n",
        "val = val.shuffle(1000)\n",
        "val = val.batch(8)\n",
        "val = val.prefetch(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "CShI2gNuWha_"
      },
      "outputs": [],
      "source": [
        "train.as_numpy_iterator().next()[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBzevNv6Wha_"
      },
      "source": [
        "###  View Images and Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "sYsob9T9WhbA"
      },
      "outputs": [],
      "source": [
        "data_samples = train.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "EfJ88j5iWhbA"
      },
      "outputs": [],
      "source": [
        "res = data_samples.next()#res[0]:images list/res[1][0]:class label list/res[1][1]:coordinations of bounding box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "FsGPCgM5WhbA"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "for idx in range(4):\n",
        "    sample_image = res[0][idx]\n",
        "    sample_coords = res[1][1][idx]\n",
        "\n",
        "    cv2.rectangle(sample_image,\n",
        "                  tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
        "                  tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)),\n",
        "                        (255,0,0), 2) #cv2. rectangle(img,x,y,rgb index) method modifies the input image by drawing a rectangle on it according to the specified parameters\n",
        "\n",
        "    ax[idx].imshow(sample_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZqC7GdXWhbB"
      },
      "source": [
        "###  Import Layers and Base Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YwFvQZ78WhbC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
        "from tensorflow.keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE_Xv9TgWhbC"
      },
      "source": [
        "###  Download VGG16 (pretrained model for object detection)\n",
        "vgg16 architecture in this link:(https://media.licdn.com/dms/image/C5612AQFcCggXwoU96A/article-inline_image-shrink_1000_1488/0/1589862621183?e=1697673600&v=beta&t=TLDAOctNTdUkmbdcW4r6AqK2gHyjajtph2nFktIbtC8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "m59URPWCWhbC"
      },
      "outputs": [],
      "source": [
        "vgg = VGG16(include_top=False) ## include_top is the part of the pretrained model wich data is flatten and is redayto go to NN and classify it and soft max the output  he we don't need this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "d2Tq8hcKWhbC"
      },
      "outputs": [],
      "source": [
        "vgg.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93Qz1zD9WhbC"
      },
      "source": [
        "###  Build instance of Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ayk35aXQWhbD"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    input_layer = Input(shape=(120,120,3))\n",
        "\n",
        "    vgg = VGG16(include_top=False)(input_layer)\n",
        "\n",
        "    # Classification Model (class label 0 or 1)\n",
        "    f1 = GlobalMaxPooling2D()(vgg)\n",
        "    class1 = Dense(2048, activation='relu')(f1)  ## input NN\n",
        "    class2 = Dense(1, activation='sigmoid')(class1)  ##OUTPUT NN\n",
        "\n",
        "\n",
        "    # Bounding box model (Regression(coordinations))\n",
        "    f2 = GlobalMaxPooling2D()(vgg)\n",
        "    regress1 = Dense(2048, activation='relu')(f2) ## input NN\n",
        "    regress2 = Dense(4, activation='sigmoid')(regress1) ##OUTPUT NN\n",
        "\n",
        "    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])\n",
        "    return facetracker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nuEY6_bWhbD"
      },
      "source": [
        "#  Test out Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Mvm5n2tlWhbD"
      },
      "outputs": [],
      "source": [
        "facetracker = build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_fDkvSgxWhbD"
      },
      "outputs": [],
      "source": [
        "facetracker.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "EDtphM3WWhbE"
      },
      "outputs": [],
      "source": [
        "X, y = train.as_numpy_iterator().next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "HEjUv4WoWhbE"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "xciEp9IEWhbF"
      },
      "outputs": [],
      "source": [
        "classes, coords = facetracker.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "b4ZdXMdLWhbF"
      },
      "outputs": [],
      "source": [
        "classes, coords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_q6p-zfWhbG"
      },
      "source": [
        "#  Define Optimizer and LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "vsJN12ziWhbG"
      },
      "outputs": [],
      "source": [
        "batches_per_epoch = len(train)\n",
        "lr_decay = (1./0.75 -1)/batches_per_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "DTEHElikWhbG"
      },
      "outputs": [],
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBLHLKazWhbG"
      },
      "source": [
        "###  Create Localization Loss and Classification Loss\n",
        " Localization Loss equation:(https://https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT62F4efuNYPXCQjSY0aEX4XqR2zYJ3FFRFYQ&usqp=CAU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "wSAls2QXWhbG"
      },
      "outputs": [],
      "source": [
        "def localization_loss(y_true, yhat):  #y_true:the real value / yhat: the predicted valu\n",
        "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2])) # tf.reduce_sum: summation / delta_coord(x,y)/ y_true: the coordination of bbox and y_true[:,:]is one row and and 4 columns(x,y,w,h) matrice\n",
        "\n",
        "    h_true = y_true[:,3] - y_true[:,1]\n",
        "    w_true = y_true[:,2] - y_true[:,0]\n",
        "\n",
        "    h_pred = yhat[:,3] - yhat[:,1] ## prediction\n",
        "    w_pred = yhat[:,2] - yhat[:,0] ## prediction\n",
        "\n",
        "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))  #delta_size(h,w)\n",
        "\n",
        "    return delta_coord + delta_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "HKw-zg8SWhbH"
      },
      "outputs": [],
      "source": [
        "classloss = tf.keras.losses.BinaryCrossentropy() ##BinaryCrossentropy The loss function which is functional in calassification issue\n",
        "regressloss = localization_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re1TBIriWhbH"
      },
      "source": [
        "# Test out Loss Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "AonWYP9mWhbH"
      },
      "outputs": [],
      "source": [
        "localization_loss(y[1], coords)  ##y[1] is coordination of bounding box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "LwMt5kf8WhbI"
      },
      "outputs": [],
      "source": [
        "classloss(y[0], classes)  ##y[0] is the binary calassification of detecting face which is 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "1ZJeKhEoWhbI"
      },
      "outputs": [],
      "source": [
        "regressloss(y[1], coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT7Yj_M5WhbI"
      },
      "source": [
        "#  Train Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT49QhyOWhbJ"
      },
      "source": [
        "###  Create Custom Model Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "R3ushTBvWhbJ"
      },
      "outputs": [],
      "source": [
        "class FaceTracker(Model):\n",
        "    def __init__(self, eyetracker,  **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.model = eyetracker\n",
        "\n",
        "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "        self.closs = classloss\n",
        "        self.lloss = localizationloss\n",
        "        self.opt = opt\n",
        "\n",
        "    def train_step(self, batch, **kwargs):\n",
        "\n",
        "        X, y = batch\n",
        "\n",
        "        with tf.GradientTape() as tape:  ##GradientTape is a mathematical tool for automatic differentiation (autodiff)\n",
        "            classes, coords = self.model(X, training=True)\n",
        "\n",
        "            batch_classloss = self.closs(y[0], classes)\n",
        "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords) #The \"tf. cast\" function casts a tensor to new type\n",
        "\n",
        "            total_loss = batch_localizationloss+0.5*batch_classloss #we have constant term behind the eqution of localizationloss which we definite it as  0.5\n",
        "\n",
        "            grad = tape.gradient(total_loss, self.model.trainable_variables)#When using gradient tape you pass model.trainable_weights which returns the weights and biases of the entire model and use the optimizer to apply the gradients.\n",
        "\n",
        "        opt.apply_gradients(zip(grad, self.model.trainable_variables)) #update the gradient of trainable variables\n",
        "\n",
        "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
        "\n",
        "    def test_step(self, batch, **kwargs):\n",
        "        X, y = batch\n",
        "\n",
        "        classes, coords = self.model(X, training=False)\n",
        "\n",
        "        batch_classloss = self.closs(y[0], classes)\n",
        "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
        "        total_loss = batch_localizationloss+0.5*batch_classloss\n",
        "\n",
        "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
        "\n",
        "    def call(self, X, **kwargs):\n",
        "        return self.model(X, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "WOGK1m_uWhbJ"
      },
      "outputs": [],
      "source": [
        "model = FaceTracker(facetracker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "waSOzfyhWhbJ"
      },
      "outputs": [],
      "source": [
        "model.compile(opt, classloss, regressloss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-QWsh3rWhbK"
      },
      "source": [
        "###  Train\n",
        "tensorboard shematic:(https://editor.analyticsvidhya.com/uploads/32892Capture.PNG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TXZbFnMgWhbK"
      },
      "outputs": [],
      "source": [
        "logdir='logs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-uyZYv-BWhbK"
      },
      "outputs": [],
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir) ##TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "CoPWAW36WhbL"
      },
      "outputs": [],
      "source": [
        "hist = model.fit(train, epochs=10, validation_data=val, callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Cxp2PoWhbL"
      },
      "source": [
        "###  Plot Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "-s1XQcPfWhbL"
      },
      "outputs": [],
      "source": [
        "hist.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "OpqxldJ-WhbM"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
        "\n",
        "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
        "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
        "ax[0].title.set_text('Loss')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
        "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
        "ax[1].title.set_text('Classification Loss')\n",
        "ax[1].legend()\n",
        "\n",
        "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
        "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
        "ax[2].title.set_text('Regression Loss')\n",
        "ax[2].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS49qcBHWhbN"
      },
      "source": [
        "###  Make Predictions on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ISFQ6wFGWhbN"
      },
      "outputs": [],
      "source": [
        "test_data = test.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "RLetKl2DWhbN"
      },
      "outputs": [],
      "source": [
        "test_sample = test_data.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "hpsjqTnPWhbO"
      },
      "outputs": [],
      "source": [
        "yhat = facetracker.predict(test_sample[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-k1sh_rhWhbO"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "for idx in range(4):\n",
        "    sample_image = test_sample[0][idx]\n",
        "    sample_coords = yhat[1][idx]# yhat[0]:class label of face/ yhat[1]:coordinaton of test_image\n",
        "\n",
        "    if yhat[0][idx] > 0.9: # if prediction of class label is bigger than 0.9  we can conclude that we have face in the image\n",
        "        cv2.rectangle(sample_image,\n",
        "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
        "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)),\n",
        "                            (255,0,0), 2)\n",
        "\n",
        "    ax[idx].imshow(sample_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNBYLoCuWhbP"
      },
      "source": [
        "###  Save the Model\n",
        "The development of the model can be saved both before and after testing. As a result, a model will pick up where it left off to eliminate lengthy training periods. You can still share your model and have others replicate it if you save it. Most machine learning professionals share the following when publishing test models and techniques:\n",
        "\n",
        "Code to create the model\n",
        "\n",
        "The trained weights for the mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "93GvEepEWhbP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "3WNrPISQWhbQ"
      },
      "outputs": [],
      "source": [
        "facetracker.save('facetracker.h5') #h5 models include the updated weights and biases in  the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "JkL7NBU9WhbQ"
      },
      "outputs": [],
      "source": [
        "facetracker = load_model('facetracker.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHyGO4uBWhbQ"
      },
      "source": [
        "###  Real Time Detection\n",
        "# **The real output in this link**:(https://user-images.githubusercontent.com/36365194/56557874-5b347e00-65ba-11e9-8b8f-48a9664b97e9.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "GE9DGf88WhbR"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(0)# now model is completed and turn the webcam on and in real time model can detect faces\n",
        "while cap.isOpened():\n",
        "    _ , frame = cap.read()\n",
        "    frame = frame[50:500, 50:500,:] #the boundary of image pixel size\n",
        "\n",
        "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)#cv2. cvtColor() method is used to convert an image from one color space to another, BGR image is converted to RGB\n",
        "\n",
        "    yhat = facetracker.predict(np.expand_dims(resized/255,0)) # numpy.expand_dims(a, axis)Expand the shape of an array.Insert a new axis that will appear at the axis position in the expanded array shape.\n",
        "    sample_coords = yhat[1]\n",
        "\n",
        "    if yhat[0] > 0.5: # check that class label bigger than 0.5 that show model can detect face by logistic regression algorithm beacuse here we have binary classification\n",
        "        # Controls the main rectangle\n",
        "        cv2.rectangle(frame,\n",
        "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
        "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)),\n",
        "                            (255,0,0), 2)\n",
        "        # Controls the label rectangle\n",
        "        cv2.rectangle(frame,\n",
        "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
        "                                    [0,-30])),\n",
        "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
        "                                    [80,0])),\n",
        "                            (255,0,0), -1)\n",
        "\n",
        "        # Controls the text rendered\n",
        "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
        "                                               [0,-5])),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA) # cv2.putText(image, text, org, font, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]])\n",
        "                                                                              #Parameters:\n",
        "                                                                              #image: It is the image on which text is to be drawn.\n",
        "                                                                              #text: Text string to be drawn.\n",
        "                                                                              #org: It is the coordinates of the bottom-left corner of the text string in the image. The coordinates are represented as tuples of two values i.e. (X coordinate value, Y coordinate value).\n",
        "                                                                              #font: It denotes the font type. Some of font types are FONT_HERSHEY_SIMPLEX, FONT_HERSHEY_PLAIN, , etc.\n",
        "                                                                              #fontScale: Font scale factor that is multiplied by the font-specific base size.\n",
        "                                                                              #color: It is the color of text string to be drawn. For BGR, we pass a tuple. eg: (255, 0, 0) for blue color.\n",
        "                                                                              #thickness: It is the thickness of the line in px.\n",
        "                                                                              #lineType: This is an optional parameter.It gives the type of the line to be used.\n",
        "                                                                              #bottomLeftOrigin: This is an optional parameter. When it is true, the image data origin is at the bottom-left corner. Otherwise, it is at the top-left corner.\n",
        "\n",
        "    cv2.imshow('EyeTrack', frame) #cv2.imshow(window_name, image)\n",
        "                                  #Parameters:\n",
        "                                  #window_name: A string representing the name of the window in which image to be displayed.\n",
        "                                  #image: It is the image that is to be displayed.\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "facedet",
      "language": "python",
      "name": "facedet"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}